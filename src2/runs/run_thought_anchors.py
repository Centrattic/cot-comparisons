"""
Run Thought Anchors on existing compression specs.

Loads specs already generated by run_compression (no re-running run_data
or LLM monitor). For each rollout, runs adaptive sentence importance
scoring, then evaluates the resulting compressed CoT.

Two evaluation modes:
  - "kl_divergence": KL(baseline || compressed). Needs baseline from
    run_compression.py.
  - "correctness": P(correct_answer | compressed_cot). No baseline needed.

Usage:
    python -m src2.runs.run_thought_anchors [question_id] [num_rollouts]
"""

import json
import sys
from pathlib import Path

from src2.methods import ThoughtAnchors
from src2.tasks import CompressedCotTask

PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent

SUBJECT_MODEL = "Qwen/Qwen3-32B"

COMPRESSION_FACTOR = 10
CHAR_LIMIT_MULTIPLIER = 1.5
COMPRESS_PCT = 0.5
REGION = "prefix"

EVAL_MODE = "kl_divergence"  # "kl_divergence" or "correctness"

QUESTION_ID = sys.argv[1] if len(sys.argv) > 1 else "starfish"
NUM_ROLLOUTS = int(sys.argv[2]) if len(sys.argv) > 2 else 25
NUM_RESAMPLES = 50

print(f"Thought Anchors: {QUESTION_ID}, {NUM_ROLLOUTS} rollouts, mode={EVAL_MODE}")

task = CompressedCotTask(
    model=SUBJECT_MODEL,
    compression_factor=COMPRESSION_FACTOR,
    char_limit_multiplier=CHAR_LIMIT_MULTIPLIER,
    compress_pct=COMPRESS_PCT,
    region=REGION,
)

# Generate any missing compression specs (no API calls, just metadata)
for ridx in range(NUM_ROLLOUTS):
    spec_dir = task.compression_dir / QUESTION_ID / f"rollout_{ridx:03d}"
    if not any(spec_dir.rglob("compression_spec.json")) if spec_dir.exists() else True:
        print(f"  Generating spec for rollout {ridx}...")
        task.run_data(question_id=QUESTION_ID, rollout_idx=ridx, verbose=False)

# Baseline only needed for KL mode
baseline = None
if EVAL_MODE == "kl_divergence":
    baseline_path = task.compression_dir / QUESTION_ID / "baseline_distribution.json"
    assert baseline_path.exists(), (
        f"No baseline found at {baseline_path} â€” run run_compression.py first"
    )
    with open(baseline_path) as f:
        baseline = json.load(f)

# Load all specs, filter to our question, deduplicate by rollout_idx (keep latest)
all_rows = task.prepare_for_thought_anchors()
seen = {}
for row in all_rows:
    if row.get("question_id") != QUESTION_ID:
        continue
    ridx = row.get("rollout_idx", -1)
    if ridx < 0 or ridx >= NUM_ROLLOUTS:
        continue
    seen[ridx] = row  # later entries overwrite earlier (sorted by time)
rows = [seen[k] for k in sorted(seen)]

assert rows, f"No specs found for {QUESTION_ID}"
print(f"Running on {len(rows)} rollouts")

anchors = ThoughtAnchors(model=SUBJECT_MODEL)
anchors.set_task(task)
anchor_results = anchors.infer(rows)
anchors._output.mark_success()

all_eval_results = []

for r in anchor_results:
    qid = r["question_id"]
    ridx = r["rollout_idx"]
    compressed_cot = r["compressed_cot"]

    print(f"\nEvaluating rollout {ridx} ({r['actual_num_selected']} sentences kept)...")

    compressed_dist = task.evaluate_compression(
        qid,
        ridx,
        compressed_cot,
        num_resamples=NUM_RESAMPLES,
    )

    if EVAL_MODE == "kl_divergence":
        metrics = task.evaluate(
            [compressed_dist["distribution"]],
            [baseline["distribution"]],
            mode=EVAL_MODE,
        )
    else:
        correct_answer = task._get_correct_answer(qid, ridx)
        metrics = task.evaluate(
            [compressed_dist["distribution"]],
            mode=EVAL_MODE,
            correct_answer=correct_answer,
        )

    eval_result = {
        "rollout_idx": ridx,
        "question_id": qid,
        "mode": EVAL_MODE,
        "num_selected": r["actual_num_selected"],
        "total_samples_used": r["total_samples_used"],
        "compressed_distribution": compressed_dist,
        **metrics,
    }
    if EVAL_MODE == "kl_divergence":
        eval_result["baseline_distribution"] = baseline
    all_eval_results.append(eval_result)

    folder = anchors.get_folder()
    with open(folder / f"eval_rollout_{ridx:03d}.json", "w") as f:
        json.dump(eval_result, f, indent=2)

    if EVAL_MODE == "kl_divergence":
        print(
            f"  KL divergence: {metrics['kl_divergence']:.4f}, "
            f"Agreement: {metrics['agreement']:.1%}"
        )
    else:
        print(f"  Correctness: {metrics['correctness']:.1%}")

if all_eval_results:
    summary_path = task.compression_dir / QUESTION_ID / "thought_anchors_eval.json"
    with open(summary_path, "w") as f:
        json.dump(all_eval_results, f, indent=2)
    print(f"\nSaved aggregate results to {summary_path}")
