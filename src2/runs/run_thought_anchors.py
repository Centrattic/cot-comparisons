"""
Run Thought Anchors on existing compression specs.

Loads specs already generated by run_compression (no re-running run_data
or LLM monitor). For each rollout, runs adaptive sentence importance
scoring, then evaluates the resulting compressed CoT.

Two evaluation modes:
  - "kl_divergence": KL(baseline || compressed). Needs baseline from
    run_compression.py.
  - "correctness": P(correct_answer | compressed_cot). No baseline needed.

Usage:
    python -m src2.runs.run_thought_anchors
"""

import json
from pathlib import Path

from src2.methods import ThoughtAnchors
from src2.tasks import CompressedCotTask

PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent

SUBJECT_MODEL = "Qwen/Qwen3-32B"

COMPRESSION_FACTOR = 10
CHAR_LIMIT_MULTIPLIER = 1.5
COMPRESS_PCT = 0.5
REGION = "prefix"

EVAL_MODE = "kl_divergence"  # "kl_divergence" or "correctness"

QUESTION_ID = "starfish"
NUM_RESAMPLES = 50

task = CompressedCotTask(
    model=SUBJECT_MODEL,
    compression_factor=COMPRESSION_FACTOR,
    char_limit_multiplier=CHAR_LIMIT_MULTIPLIER,
    compress_pct=COMPRESS_PCT,
    region=REGION,
)

# Baseline only needed for KL mode
baseline = None
if EVAL_MODE == "kl_divergence":
    baseline_path = task.compression_dir / QUESTION_ID / "baseline_distribution.json"
    assert baseline_path.exists(), (
        f"No baseline found at {baseline_path} — run run_compression.py first"
    )
    with open(baseline_path) as f:
        baseline = json.load(f)

all_rows = task.prepare_for_thought_anchors()
assert all_rows, "No compression specs found — run run_compression.py first"
print(f"Found {len(all_rows)} rollout specs")

anchors = ThoughtAnchors(model=SUBJECT_MODEL)
anchors.set_task(task)
anchor_results = anchors.infer(all_rows)
anchors._output.mark_success()

all_eval_results = []

for r in anchor_results:
    qid = r["question_id"]
    ridx = r["rollout_idx"]
    compressed_cot = r["compressed_cot"]

    print(f"\nEvaluating rollout {ridx} ({r['actual_num_selected']} sentences kept)...")

    compressed_dist = task.evaluate_compression(
        qid,
        ridx,
        compressed_cot,
        num_resamples=NUM_RESAMPLES,
    )

    if EVAL_MODE == "kl_divergence":
        metrics = task.evaluate(
            [compressed_dist["distribution"]],
            [baseline["distribution"]],
            mode=EVAL_MODE,
        )
    else:
        correct_answer = task._get_correct_answer(qid, ridx)
        metrics = task.evaluate(
            [compressed_dist["distribution"]],
            mode=EVAL_MODE,
            correct_answer=correct_answer,
        )

    eval_result = {
        "rollout_idx": ridx,
        "question_id": qid,
        "mode": EVAL_MODE,
        "num_selected": r["actual_num_selected"],
        "total_samples_used": r["total_samples_used"],
        "compressed_distribution": compressed_dist,
        **metrics,
    }
    if EVAL_MODE == "kl_divergence":
        eval_result["baseline_distribution"] = baseline
    all_eval_results.append(eval_result)

    folder = anchors.get_folder()
    with open(folder / f"eval_rollout_{ridx:03d}.json", "w") as f:
        json.dump(eval_result, f, indent=2)

    if EVAL_MODE == "kl_divergence":
        print(
            f"  KL divergence: {metrics['kl_divergence']:.4f}, "
            f"Agreement: {metrics['agreement']:.1%}"
        )
    else:
        print(f"  Correctness: {metrics['correctness']:.1%}")

if all_eval_results:
    summary_path = task.compression_dir / QUESTION_ID / "thought_anchors_eval.json"
    with open(summary_path, "w") as f:
        json.dump(all_eval_results, f, indent=2)
    print(f"\nSaved aggregate results to {summary_path}")
